{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImbXGZv-Ji69",
        "outputId": "0373fabe-cda4-4daa-ed4d-e95eaacff39d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting environment setup...\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Selecting previously unselected package openjdk-11-jre-headless:amd64.\n",
            "(Reading database ... 121703 files and directories currently installed.)\n",
            "Preparing to unpack .../openjdk-11-jre-headless_11.0.28+6-1ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-11-jre-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "Selecting previously unselected package openjdk-11-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-11-jdk-headless_11.0.28+6-1ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-11-jdk-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "Setting up openjdk-11-jre-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jjs to provide /usr/bin/jjs (jjs) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmid to provide /usr/bin/rmid (rmid) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/pack200 to provide /usr/bin/pack200 (pack200) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/unpack200 to provide /usr/bin/unpack200 (unpack200) in auto mode\n",
            "Setting up openjdk-11-jdk-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmic to provide /usr/bin/rmic (rmic) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jaotc to provide /usr/bin/jaotc (jaotc) in auto mode\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dataproc-spark-connect 0.8.3 requires pyspark[connect]~=3.5.1, but you have pyspark 3.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mPackages installed. Importing modules...\n"
          ]
        }
      ],
      "source": [
        "# 1) ENVIRONMENT SETUP (run in Colab)\n",
        "# - Installs Java + PySpark + other Python libs\n",
        "# - Recommended for Colab environment\n",
        "import sys\n",
        "import time\n",
        "from IPython.display import clear_output, display\n",
        "print(\"Starting environment setup...\")\n",
        "\n",
        "# Install system and Python packages - these commands run in the shell (Colab)\n",
        "# Note: In Colab, prefix shell commands with \"!\" — the notebook will allow it.\n",
        "# If you run this locally, adapt install commands appropriately.\n",
        "!apt-get update -qq\n",
        "!apt-get install -y -qq openjdk-11-jdk-headless\n",
        "!pip install -q pyspark==3.4.1\n",
        "!pip install -q requests beautifulsoup4 lxml matplotlib seaborn\n",
        "\n",
        "print(\"Packages installed. Importing modules...\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard imports (must be after pip installs when running in a fresh Colab runtime)\n",
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.functions import udf, col"
      ],
      "metadata": {
        "id": "WnOvCF0TJj8Z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #2) START SPARK SESSION\n",
        "# ---------------------------\n",
        "# Create a SparkSession suitable for Colab usage\n",
        "# Adjust appName / master as needed. This uses local mode.\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"ColabNewsSentiment\") \\\n",
        "    .config(\"spark.ui.showConsoleProgress\", \"false\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "sc.setLogLevel(\"ERROR\")\n",
        "print(\"Spark session started:\", spark)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbmaSIVaJp1l",
        "outputId": "c7b341e2-a849-4457-c3ce-48641b067439"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark session started: <pyspark.sql.session.SparkSession object at 0x798dc1aff4d0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) NEWS FETCHING (Real-Time Simulation)\n",
        "# ---------------------------\n",
        "# Strategy:\n",
        "# - Default: fetch latest headlines from a public RSS feed (no API key needed).\n",
        "#   Example: BBC News RSS feed: http://feeds.bbci.co.uk/news/rss.xml\n",
        "# - Alternate (commented) option: use NewsAPI (requires an API key) or other paid services.\n",
        "#\n",
        "# Where to put API key: if you choose to use NewsAPI, place your key in NEWSAPI_KEY variable\n",
        "# (see commented example below).\n",
        "#\n",
        "# Implementation: fetch_headlines() -> returns list of headline strings\n",
        "\n",
        "def fetch_headlines_from_rss(rss_url=\"http://feeds.bbci.co.uk/news/rss.xml\", max_items=50):\n",
        "    \"\"\"\n",
        "    Fetches headlines from an RSS feed URL (public, no key).\n",
        "    Returns a Python list of headline strings.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        resp = requests.get(rss_url, timeout=10)\n",
        "        resp.raise_for_status()\n",
        "        soup = BeautifulSoup(resp.content, \"xml\")\n",
        "        items = soup.find_all(\"item\")\n",
        "        headlines = []\n",
        "        for it in items[:max_items]:\n",
        "            title_tag = it.find(\"title\")\n",
        "            if title_tag:\n",
        "                headlines.append(title_tag.get_text().strip())\n",
        "        return headlines\n",
        "    except Exception as e:\n",
        "        print(\"RSS fetch failed:\", e)\n",
        "        return []\n",
        "\n",
        "# Example: how to use a real News API (commented)\n",
        "# NEWSAPI_KEY = \"<YOUR_NEWSAPI_KEY>\"  # <-- put key here if you want to use NewsAPI\n",
        "# def fetch_with_newsapi(query=\"world\", page_size=50):\n",
        "#     url = f\"https://newsapi.org/v2/everything?q={query}&pageSize={page_size}&apiKey={NEWSAPI_KEY}\"\n",
        "#     r = requests.get(url)\n",
        "#     data = r.json()\n",
        "#     return [a['title'] for a in data.get('articles', [])]\n",
        "\n",
        "# Initial fetch to seed headlines:\n",
        "seed_headlines = fetch_headlines_from_rss(max_items=100)\n",
        "print(f\"Fetched {len(seed_headlines)} seed headlines (sample):\")\n",
        "for h in seed_headlines[:10]:\n",
        "    print(\" -\", h)\n",
        "\n",
        "# If no headlines were fetched (network issues), fallback to a small synthetic list:\n",
        "if len(seed_headlines) == 0:\n",
        "    print(\"No RSS headlines found - falling back to synthetic headlines for streaming.\")\n",
        "    seed_headlines = [\n",
        "        \"Government announces new economic measures to boost growth\",\n",
        "        \"Local team wins dramatic overtime victory in championship\",\n",
        "        \"Company reports mixed quarterly results as markets react\",\n",
        "        \"Severe weather warnings issued across coastal regions\",\n",
        "        \"Tech startup raises funds for AI-driven healthcare app\",\n",
        "        \"Factory explosion injures several workers in industrial zone\",\n",
        "        \"Scientists report breakthrough in renewable energy storage\",\n",
        "        \"Protests erupt after controversial court ruling\"\n",
        "    ]\n",
        "\n",
        "# ---------------------------\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SLul9heJuX1",
        "outputId": "904f3693-ed1f-4437-a23d-584a198ed527"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetched 38 seed headlines (sample):\n",
            " - Google boss warns 'no company is going to be immune' if AI bubble bursts\n",
            " - Don't blindly trust what AI tells you, says Google's Sundar Pichai\n",
            " - StubHub and Wayfair among firms named in online pricing investigation\n",
            " - Unprecedented plan for asylum system sees government walk tightrope\n",
            " - So long, plastic wet wipes - but should we be flushing the new ones?\n",
            " - KPop Demon Hunters star on how her life mirrored main character's journey\n",
            " - Paralegal sacked after offering to help dodge £60k illegal working fines\n",
            " - Dan Wootton denies High Court claim that he catfished 'former colleague'\n",
            " - Trump's plan for Gaza backed by UN Security Council\n",
            " - Shelters plea for Gazans as winter rains raise fears of more disease and death\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) SYNTHETIC TRAINING DATA (small dataset generated programmatically)\n",
        "# ---------------------------\n",
        "# We create a small dataset of labeled headlines (Positive/Negative) programmatically,\n",
        "# using templates to produce a few hundred examples quickly and reproducibly.\n",
        "# This keeps the notebook self-contained and runnable immediately.\n",
        "\n",
        "positive_templates = [\n",
        "    \"growth\", \"wins\", \"soars\", \"improves\", \"record high\", \"surge\", \"positive outlook\",\n",
        "    \"successful\", \"recovery\", \"gain\", \"reform\", \"approval\", \"solves\", \"breakthrough\", \"expand\"\n",
        "]\n",
        "negative_templates = [\n",
        "    \"loss\", \"crash\", \"decline\", \"fails\", \"downturn\", \"scandal\", \"controversy\", \"protest\",\n",
        "    \"injures\", \"dies\", \"arrested\", \"attack\", \"warning\", \"concern\", \"shortage\"\n",
        "]\n",
        "\n",
        "def generate_synthetic_headlines(n=300):\n",
        "    \"\"\"\n",
        "    Generate n synthetic (headline, label) tuples.\n",
        "    Label is 'Positive' or 'Negative'.\n",
        "    \"\"\"\n",
        "    samples = []\n",
        "    for i in range(n):\n",
        "        if random.random() < 0.5:\n",
        "            # Positive\n",
        "            t = random.choice(positive_templates)\n",
        "            headline = f\"{random.choice(['Company','Market','Team','Government','City'])} {t} as {random.choice(['demand rises','policy works','efforts pay off','confidence returns'])}\"\n",
        "            label = \"Positive\"\n",
        "        else:\n",
        "            # Negative\n",
        "            t = random.choice(negative_templates)\n",
        "            headline = f\"{random.choice(['Company','Market','Region','Hospital','Agency'])} {t} after {random.choice(['incident','reports','investigation','shortage','accident'])}\"\n",
        "            label = \"Negative\"\n",
        "        samples.append((headline, label))\n",
        "    return samples\n",
        "\n",
        "# Generate synthetic training data\n",
        "synthetic_data = generate_synthetic_headlines(n=400)\n",
        "print(\"Generated synthetic training dataset size:\", len(synthetic_data))\n",
        "print(\"Sample examples:\")\n",
        "for s in synthetic_data[:6]:\n",
        "    print(\" -\", s)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-brzDACJ051",
        "outputId": "147aacd6-4c20-47e6-c830-1e0bba20efd6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated synthetic training dataset size: 400\n",
            "Sample examples:\n",
            " - ('Government approval as efforts pay off', 'Positive')\n",
            " - ('Government expand as demand rises', 'Positive')\n",
            " - ('Region arrested after accident', 'Negative')\n",
            " - ('Region warning after reports', 'Negative')\n",
            " - ('Team breakthrough as confidence returns', 'Positive')\n",
            " - ('City breakthrough as policy works', 'Positive')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) CREATE TRAINING SPARK DATAFRAME AND TRAIN PySpark ML PIPELINE\n",
        "# ---------------------------\n",
        "from pyspark.sql import Row\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Create Spark DataFrame from synthetic_data\n",
        "rows = [Row(text=txt, label=lab) for txt, lab in synthetic_data]\n",
        "train_df = spark.createDataFrame(rows)\n",
        "train_df = train_df.select(col(\"text\"), col(\"label\"))\n",
        "\n",
        "# Convert string labels (\"Positive\"/\"Negative\") into numeric index for ML algorithms\n",
        "label_indexer = StringIndexer(inputCol=\"label\", outputCol=\"labelIndex\")\n",
        "\n",
        "# Build a pipeline: Tokenizer -> StopWordsRemover -> HashingTF -> IDF -> LogisticRegression\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
        "stop_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
        "hashing_tf = HashingTF(inputCol=\"filtered_tokens\", outputCol=\"rawFeatures\", numFeatures=1 << 12)  # 4096\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"labelIndex\", maxIter=50)\n",
        "\n",
        "pipeline = Pipeline(stages=[label_indexer, tokenizer, stop_remover, hashing_tf, idf, lr])\n",
        "\n",
        "# Fit the pipeline\n",
        "print(\"Training pipeline on synthetic data (this may take a few seconds)...\")\n",
        "pipeline_model = pipeline.fit(train_df)\n",
        "print(\"Pipeline trained.\")\n",
        "\n",
        "# Evaluate quickly on training data (sanity-check)\n",
        "preds = pipeline_model.transform(train_df)\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"labelIndex\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "acc = evaluator.evaluate(preds)\n",
        "print(f\"Training accuracy (synthetic data): {acc:.3f}\")\n",
        "\n",
        "# Map numerical predictions back to string labels:\n",
        "# We need to know how StringIndexer mapped \"Positive\"/\"Negative\" -> numeric\n",
        "label_mapping = pipeline_model.stages[0].labels  # list like ['Negative','Positive'] or vice versa\n",
        "print(\"Label mapping (index -> label):\", list(enumerate(label_mapping)))\n",
        "\n",
        "def index_to_label(idx):\n",
        "    # helper: convert numeric index to label string\n",
        "    return label_mapping[int(idx)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3A6Aa_pMJ35i",
        "outputId": "8a08297c-f22c-4afc-8d9e-63cd851a4202"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training pipeline on synthetic data (this may take a few seconds)...\n",
            "Pipeline trained.\n",
            "Training accuracy (synthetic data): 1.000\n",
            "Label mapping (index -> label): [(0, 'Positive'), (1, 'Negative')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6) STREAMING SIMULATION: readStream('rate') + foreachBatch to apply pipeline_model\n",
        "# ---------------------------\n",
        "# Approach:\n",
        "# - Use Spark Structured Streaming 'rate' source to generate sequential 'value' numbers.\n",
        "# - In foreachBatch (which runs on driver), fetch latest headlines (using RSS), map value -> headline\n",
        "# - Create a small batch DataFrame with columns 'text' and process it with pipeline_model.transform (batch-mode)\n",
        "# - Aggregate results into a driver-side list for visualization\n",
        "#\n",
        "# Note: We intentionally use foreachBatch to keep model application simple and robust with PySpark ML.\n",
        "\n",
        "from pyspark.sql.functions import expr\n",
        "\n",
        "# Global store updated within foreachBatch; driver-only list for visualization\n",
        "# Each element will be a dict with counts and timestamp\n",
        "results_store = []  # driver-side list to store results per micro-batch\n",
        "\n",
        "# Size of window for mapping rate.value -> headline index\n",
        "headline_pool = seed_headlines.copy()  # initial seed headlines; foreachBatch will optionally refresh\n",
        "headline_pool_len = len(headline_pool)\n",
        "print(\"Using headline pool size:\", headline_pool_len)\n",
        "\n",
        "def foreach_batch_function(batch_df, batch_id):\n",
        "    \"\"\"\n",
        "    This function will be called for each micro-batch by Structured Streaming.\n",
        "    It runs on the driver; we:\n",
        "      - fetch latest headlines (optional)\n",
        "      - produce a list of headlines mapped from the 'value' column\n",
        "      - create a Spark DataFrame of headlines and apply the trained pipeline_model\n",
        "      - aggregate counts of Positive/Negative and append to results_store\n",
        "    \"\"\"\n",
        "    global results_store, headline_pool, headline_pool_len\n",
        "\n",
        "    # Convert micro-batch to Pandas to get 'value' list (small micro-batches expected)\n",
        "    try:\n",
        "        values = [int(r.value) for r in batch_df.select(\"value\").collect()]\n",
        "    except Exception as e:\n",
        "        print(\"Error collecting batch values:\", e)\n",
        "        return\n",
        "\n",
        "    if len(values) == 0:\n",
        "        # nothing in this micro-batch\n",
        "        return\n",
        "\n",
        "    # Optionally fetch latest headlines from RSS on each batch to simulate 'real-time' new headlines\n",
        "    try:\n",
        "        fresh = fetch_headlines_from_rss(max_items=100)\n",
        "        if len(fresh) > 0:\n",
        "            headline_pool = fresh\n",
        "            headline_pool_len = len(headline_pool)\n",
        "    except Exception as e:\n",
        "        # ignore fetch errors, keep existing pool\n",
        "        pass\n",
        "\n",
        "    # Map rate 'value's to headlines via modulo indexing\n",
        "    batch_headlines = []\n",
        "    for v in values:\n",
        "        idx = int(v % headline_pool_len)\n",
        "        batch_headlines.append((str(headline_pool[idx]),))  # single-column tuple\n",
        "\n",
        "    # Create batch DataFrame with column 'text'\n",
        "    batch_rows = [Row(text=h[0]) for h in batch_headlines]\n",
        "    if len(batch_rows) == 0:\n",
        "        return\n",
        "    micro_df = spark.createDataFrame(batch_rows)\n",
        "\n",
        "    # If pipeline_model expects a label column (it does during training) it's okay:\n",
        "    # model.transform will produce predictions based on features.\n",
        "    transformed = pipeline_model.transform(micro_df)\n",
        "\n",
        "    # Collect predictions and compute counts\n",
        "    pred_rows = transformed.select(\"text\", \"prediction\").collect()\n",
        "    pos_count = 0\n",
        "    neg_count = 0\n",
        "    details = []\n",
        "    for r in pred_rows:\n",
        "        label = index_to_label(r.prediction)\n",
        "        details.append((r.text, label))\n",
        "        if label == \"Positive\":\n",
        "            pos_count += 1\n",
        "        else:\n",
        "            neg_count += 1\n",
        "\n",
        "    # Append aggregated results (timestamp, counts, details) to results_store\n",
        "    batch_time = time.time()\n",
        "    results_store.append({\n",
        "        \"batch_id\": int(batch_id),\n",
        "        \"ts\": batch_time,\n",
        "        \"positive\": pos_count,\n",
        "        \"negative\": neg_count,\n",
        "        \"total\": pos_count + neg_count,\n",
        "        \"details\": details\n",
        "    })\n",
        "\n",
        "    # For debug: print small summary\n",
        "    print(f\"[batch {batch_id}] +{pos_count} pos / {neg_count} neg (total {pos_count + neg_count})\")\n",
        "\n",
        "# Create a streaming DataFrame using 'rate' to produce rows at runtime\n",
        "# Each row has schema: timestamp, value\n",
        "streaming_input = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 2).load()\n",
        "\n",
        "# Start streaming query with foreachBatch sink\n",
        "query = streaming_input.writeStream.foreachBatch(foreach_batch_function).start()\n",
        "print(\"Structured streaming query started. Query id:\", query.id)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_P-8q2hnJ7Ea",
        "outputId": "48e75788-d26b-465f-b380-c77acbe7ab5f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using headline pool size: 38\n",
            "Structured streaming query started. Query id: f253b319-0295-47bb-a16a-d51a6b3750a6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7) REAL-TIME DASHBOARD (driver-side)\n",
        "# ---------------------------\n",
        "# We'll run a short live visualization loop that polls results_store and plots Positive vs Negative.\n",
        "# The streaming query is running in background; foreachBatch will append to results_store regularly.\n",
        "\n",
        "print(\"Starting live dashboard (will poll for 30 seconds). Press interrupt to stop sooner.\")\n",
        "\n",
        "display_interval = 1.0  # seconds between dashboard refresh\n",
        "dashboard_duration = 30  # seconds for demonstration (adjust as desired)\n",
        "\n",
        "start_time = time.time()\n",
        "try:\n",
        "    while time.time() - start_time < dashboard_duration:\n",
        "        # Build aggregated counts from results_store\n",
        "        total_pos = sum(item['positive'] for item in results_store)\n",
        "        total_neg = sum(item['negative'] for item in results_store)\n",
        "        total = total_pos + total_neg\n",
        "\n",
        "        clear_output(wait=True)\n",
        "        print(f\"Real-time News Sentiment Dashboard (simulated) — running for {int(time.time()-start_time)}s\")\n",
        "        print(f\"Total processed headlines: {total}    Positive: {total_pos}    Negative: {total_neg}\\n\")\n",
        "\n",
        "        # Plot counts as a bar chart\n",
        "        fig, ax = plt.subplots(figsize=(6,3))\n",
        "        sns.barplot(x=[\"Positive\", \"Negative\"], y=[total_pos, total_neg], ax=ax)\n",
        "        ax.set_ylim(0, max(5, total_pos + total_neg))\n",
        "        ax.set_title(\"Cumulative Sentiment Counts (simulated real-time)\")\n",
        "        for i, v in enumerate([total_pos, total_neg]):\n",
        "            ax.text(i, v + 0.05, str(v), ha=\"center\")\n",
        "        plt.show()\n",
        "\n",
        "        # Print last batch details (if present)\n",
        "        if len(results_store) > 0:\n",
        "            last = results_store[-1]\n",
        "            print(f\"Last batch (id={last['batch_id']}) processed at {time.ctime(last['ts'])}: \"\n",
        "                  f\"{last['positive']} positive, {last['negative']} negative\")\n",
        "            print(\"Sample classifications from last batch:\")\n",
        "            for sample_text, sample_label in last[\"details\"][:5]:\n",
        "                print(\" -\", sample_label, \":\", sample_text)\n",
        "\n",
        "        time.sleep(display_interval)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Dashboard interrupted by user.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "URa57pqnJ9t3",
        "outputId": "e9d307b9-5e39-4636-ce03-045d82b79677"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Real-time News Sentiment Dashboard (simulated) — running for 11s\n",
            "Total processed headlines: 24    Positive: 2    Negative: 22\n",
            "\n",
            "Dashboard interrupted by user.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Spark ML provides its own methods for saving and loading models\n",
        "pipeline_model.save(\"sentiment_pipeline_model\")\n",
        "\n",
        "print(\"✅ Pipeline model saved successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8T7A1WlKAn0",
        "outputId": "977d7625-2fa6-41ae-fea7-12a6e38bda47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[batch 1491] +0 pos / 2 neg (total 2)\n",
            "[batch 1492] +0 pos / 2 neg (total 2)\n",
            "[batch 1493] +0 pos / 2 neg (total 2)\n",
            "[batch 1494] +1 pos / 1 neg (total 2)\n",
            "[batch 1495] +1 pos / 1 neg (total 2)\n",
            "[batch 1496] +0 pos / 2 neg (total 2)\n",
            "✅ Pipeline model saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install vaderSentiment\n",
        "\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Create the analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def get_sentiment(news_text):\n",
        "    scores = analyzer.polarity_scores(news_text)\n",
        "    compound = scores[\"compound\"]\n",
        "\n",
        "    if compound >= 0.05:\n",
        "        return \"Positive\", scores\n",
        "    elif compound <= -0.05:\n",
        "        return \"Negative\", scores\n",
        "    else:\n",
        "        return \"Neutral\", scores\n",
        "\n",
        "\n",
        "# Test\n",
        "while True:\n",
        "    news = input(\"\\nEnter a news headline (or type 'exit'): \")\n",
        "    if news.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    sentiment, score = get_sentiment(news)\n",
        "    print(f\"\\nSentiment: {sentiment}\")\n",
        "    print(f\"Scores: {score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVNFxOFQVkrA",
        "outputId": "931e0f2f-83a3-4abc-dd95-6041d3be8a52"
      },
      "execution_count": 9,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[batch 19] +0 pos / 2 neg (total 2)\n",
            "[batch 20] +0 pos / 2 neg (total 2)\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from vaderSentiment) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (2025.10.5)\n",
            "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h[batch 21] +0 pos / 2 neg (total 2)\n",
            "[batch 22] +0 pos / 2 neg (total 2)\n",
            "[batch 23] +1 pos / 3 neg (total 4)\n",
            "[batch 24] +0 pos / 2 neg (total 2)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n",
            "[batch 25] +0 pos / 2 neg (total 2)\n",
            "[batch 26] +0 pos / 2 neg (total 2)\n",
            "[batch 27] +0 pos / 2 neg (total 2)\n",
            "[batch 28] +0 pos / 2 neg (total 2)\n",
            "[batch 29] +0 pos / 4 neg (total 4)\n",
            "[batch 30] +0 pos / 2 neg (total 2)\n",
            "[batch 31] +0 pos / 2 neg (total 2)\n",
            "[batch 32] +0 pos / 2 neg (total 2)\n",
            "[batch 33] +0 pos / 2 neg (total 2)\n",
            "[batch 34] +1 pos / 1 neg (total 2)\n",
            "[batch 35] +0 pos / 2 neg (total 2)\n",
            "[batch 36] +0 pos / 2 neg (total 2)\n",
            "[batch 37] +0 pos / 2 neg (total 2)\n",
            "[batch 38] +0 pos / 2 neg (total 2)\n",
            "[batch 39] +0 pos / 2 neg (total 2)\n",
            "[batch 40] +0 pos / 2 neg (total 2)\n",
            "[batch 41] +1 pos / 1 neg (total 2)\n",
            "[batch 42] +0 pos / 2 neg (total 2)\n",
            "[batch 43] +0 pos / 2 neg (total 2)\n",
            "[batch 44] +0 pos / 2 neg (total 2)\n",
            "[batch 45] +0 pos / 2 neg (total 2)\n",
            "[batch 46] +0 pos / 2 neg (total 2)\n",
            "\n",
            "Enter a news headline (or type 'exit'): Stock prices rise as market confidence increases\n",
            "[batch 47] +0 pos / 2 neg (total 2)\n",
            "\n",
            "Sentiment: Positive\n",
            "Scores: {'neg': 0.0, 'neu': 0.645, 'pos': 0.355, 'compound': 0.5106}\n",
            "[batch 48] +0 pos / 2 neg (total 2)\n",
            "[batch 49] +0 pos / 2 neg (total 2)\n",
            "[batch 50] +0 pos / 2 neg (total 2)\n",
            "[batch 51] +0 pos / 2 neg (total 2)\n",
            "[batch 52] +0 pos / 2 neg (total 2)\n",
            "[batch 53] +1 pos / 1 neg (total 2)\n",
            "[batch 54] +0 pos / 2 neg (total 2)\n",
            "[batch 55] +0 pos / 2 neg (total 2)\n",
            "[batch 56] +0 pos / 2 neg (total 2)\n",
            "[batch 57] +0 pos / 2 neg (total 2)\n",
            "[batch 58] +0 pos / 2 neg (total 2)\n",
            "[batch 59] +0 pos / 2 neg (total 2)\n",
            "[batch 60] +1 pos / 1 neg (total 2)\n",
            "[batch 61] +0 pos / 2 neg (total 2)\n",
            "[batch 62] +0 pos / 2 neg (total 2)\n",
            "[batch 63] +0 pos / 2 neg (total 2)\n",
            "[batch 64] +0 pos / 2 neg (total 2)\n",
            "[batch 65] +0 pos / 2 neg (total 2)\n",
            "[batch 66] +0 pos / 2 neg (total 2)\n",
            "[batch 67] +0 pos / 2 neg (total 2)\n",
            "[batch 68] +0 pos / 2 neg (total 2)\n",
            "[batch 69] +0 pos / 2 neg (total 2)\n",
            "[batch 70] +0 pos / 2 neg (total 2)\n",
            "\n",
            "Enter a news headline (or type 'exit'): “New breakthrough in cancer research brings hope to millions.”\n",
            "[batch 71] +0 pos / 2 neg (total 2)\n",
            "\n",
            "Sentiment: Negative\n",
            "Scores: {'neg': 0.308, 'neu': 0.49, 'pos': 0.203, 'compound': -0.3612}\n",
            "[batch 72] +1 pos / 1 neg (total 2)\n",
            "[batch 73] +0 pos / 2 neg (total 2)\n",
            "[batch 74] +0 pos / 2 neg (total 2)\n",
            "[batch 75] +0 pos / 2 neg (total 2)\n",
            "[batch 76] +0 pos / 2 neg (total 2)\n",
            "[batch 77] +0 pos / 2 neg (total 2)\n",
            "[batch 78] +0 pos / 2 neg (total 2)\n",
            "[batch 79] +1 pos / 1 neg (total 2)\n",
            "[batch 80] +0 pos / 2 neg (total 2)\n",
            "[batch 81] +0 pos / 2 neg (total 2)\n",
            "[batch 82] +0 pos / 2 neg (total 2)\n",
            "[batch 83] +0 pos / 2 neg (total 2)\n",
            "[batch 84] +0 pos / 2 neg (total 2)\n",
            "[batch 85] +0 pos / 2 neg (total 2)\n",
            "[batch 86] +0 pos / 2 neg (total 2)\n",
            "[batch 87] +0 pos / 2 neg (total 2)\n",
            "\n",
            "Enter a news headline (or type 'exit'): “Tech company announces record profits in the last quarter.”\n",
            "[batch 88] +0 pos / 2 neg (total 2)\n",
            "\n",
            "Sentiment: Positive\n",
            "Scores: {'neg': 0.0, 'neu': 0.734, 'pos': 0.266, 'compound': 0.4404}\n",
            "[batch 89] +0 pos / 2 neg (total 2)\n",
            "[batch 90] +0 pos / 2 neg (total 2)\n",
            "[batch 91] +1 pos / 1 neg (total 2)\n",
            "[batch 92] +0 pos / 2 neg (total 2)\n",
            "[batch 93] +0 pos / 2 neg (total 2)\n",
            "[batch 94] +0 pos / 2 neg (total 2)\n",
            "[batch 95] +0 pos / 2 neg (total 2)\n",
            "[batch 96] +0 pos / 2 neg (total 2)\n",
            "[batch 97] +0 pos / 2 neg (total 2)\n",
            "[batch 98] +1 pos / 1 neg (total 2)\n",
            "\n",
            "Enter a news headline (or type 'exit'): “India’s space mission successfully deploys satellite into orbit.”\n",
            "[batch 99] +0 pos / 2 neg (total 2)\n",
            "\n",
            "Sentiment: Positive\n",
            "Scores: {'neg': 0.0, 'neu': 0.686, 'pos': 0.314, 'compound': 0.4939}\n",
            "[batch 100] +0 pos / 2 neg (total 2)\n",
            "[batch 101] +0 pos / 2 neg (total 2)\n",
            "[batch 102] +0 pos / 2 neg (total 2)\n",
            "[batch 103] +0 pos / 2 neg (total 2)\n",
            "[batch 104] +0 pos / 2 neg (total 2)\n",
            "[batch 105] +0 pos / 2 neg (total 2)\n",
            "[batch 106] +0 pos / 2 neg (total 2)\n",
            "[batch 107] +0 pos / 2 neg (total 2)\n",
            "\n",
            "Enter a news headline (or type 'exit'): “Company faces heavy losses after product recall controversy.”\n",
            "[batch 108] +0 pos / 2 neg (total 2)\n",
            "\n",
            "Sentiment: Negative\n",
            "Scores: {'neg': 0.278, 'neu': 0.722, 'pos': 0.0, 'compound': -0.4019}\n",
            "[batch 109] +0 pos / 2 neg (total 2)\n",
            "[batch 110] +1 pos / 1 neg (total 2)\n",
            "[batch 111] +0 pos / 2 neg (total 2)\n",
            "[batch 112] +0 pos / 2 neg (total 2)\n",
            "[batch 113] +0 pos / 2 neg (total 2)\n",
            "[batch 114] +0 pos / 2 neg (total 2)\n",
            "[batch 115] +0 pos / 2 neg (total 2)\n",
            "[batch 116] +0 pos / 2 neg (total 2)\n",
            "[batch 117] +1 pos / 1 neg (total 2)\n",
            "[batch 118] +0 pos / 2 neg (total 2)\n",
            "\n",
            "Enter a news headline (or type 'exit'): “Hospital reports rise in dengue cases across the city.”\n",
            "\n",
            "Sentiment: Neutral\n",
            "Scores: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "[batch 119] +0 pos / 2 neg (total 2)\n",
            "[batch 120] +0 pos / 2 neg (total 2)\n",
            "[batch 121] +0 pos / 2 neg (total 2)\n",
            "[batch 122] +0 pos / 2 neg (total 2)\n",
            "[batch 123] +0 pos / 2 neg (total 2)\n",
            "[batch 124] +0 pos / 2 neg (total 2)\n",
            "[batch 125] +0 pos / 2 neg (total 2)\n",
            "[batch 126] +0 pos / 2 neg (total 2)\n",
            "[batch 127] +0 pos / 2 neg (total 2)\n",
            "\n",
            "Enter a news headline (or type 'exit'): “Major data breach exposes private information of millions.”\n",
            "\n",
            "Sentiment: Negative\n",
            "Scores: {'neg': 0.176, 'neu': 0.824, 'pos': 0.0, 'compound': -0.128}\n",
            "[batch 128] +0 pos / 2 neg (total 2)\n",
            "[batch 129] +1 pos / 1 neg (total 2)\n",
            "[batch 130] +0 pos / 2 neg (total 2)\n",
            "[batch 131] +0 pos / 2 neg (total 2)\n",
            "[batch 132] +0 pos / 2 neg (total 2)\n",
            "[batch 133] +0 pos / 2 neg (total 2)\n",
            "[batch 134] +0 pos / 2 neg (total 2)\n",
            "[batch 135] +0 pos / 2 neg (total 2)\n",
            "\n",
            "Enter a news headline (or type 'exit'): “Traffic regulations to be updated starting next week.”\n",
            "\n",
            "Sentiment: Neutral\n",
            "Scores: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "[batch 136] +1 pos / 1 neg (total 2)\n",
            "[batch 137] +0 pos / 2 neg (total 2)\n",
            "[batch 138] +0 pos / 2 neg (total 2)\n",
            "[batch 139] +0 pos / 2 neg (total 2)\n",
            "[batch 140] +0 pos / 2 neg (total 2)\n",
            "[batch 141] +0 pos / 2 neg (total 2)\n",
            "[batch 142] +0 pos / 2 neg (total 2)\n",
            "\n",
            "Enter a news headline (or type 'exit'): “Scientists continue to monitor weather patterns in the region.”\n",
            "[batch 143] +0 pos / 2 neg (total 2)\n",
            "\n",
            "Sentiment: Neutral\n",
            "Scores: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "[batch 144] +0 pos / 2 neg (total 2)\n",
            "[batch 145] +0 pos / 2 neg (total 2)\n",
            "[batch 146] +0 pos / 2 neg (total 2)\n",
            "\n",
            "Enter a news headline (or type 'exit'): exit\n"
          ]
        }
      ]
    }
  ]
}