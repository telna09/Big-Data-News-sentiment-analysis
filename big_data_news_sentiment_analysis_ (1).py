# -*- coding: utf-8 -*-
"""Big data News sentiment analysis .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gVG_7JsaGGD3SSWC1HzgRfsIK6qCgWCn
"""

# 1) ENVIRONMENT SETUP (run in Colab)
# - Installs Java + PySpark + other Python libs
# - Recommended for Colab environment
import sys
import time
from IPython.display import clear_output, display
print("Starting environment setup...")

# Install system and Python packages - these commands run in the shell (Colab)
# Note: In Colab, prefix shell commands with "!" â€” the notebook will allow it.
# If you run this locally, adapt install commands appropriately.
!apt-get update -qq
!apt-get install -y -qq openjdk-11-jdk-headless
!pip install -q pyspark==3.4.1
!pip install -q requests beautifulsoup4 lxml matplotlib seaborn

print("Packages installed. Importing modules...")

# Standard imports (must be after pip installs when running in a fresh Colab runtime)
import os
import requests
from bs4 import BeautifulSoup
import random
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pyspark.sql import SparkSession
from pyspark.sql.types import StringType
from pyspark.sql.functions import udf, col

#2) START SPARK SESSION
# ---------------------------
# Create a SparkSession suitable for Colab usage
# Adjust appName / master as needed. This uses local mode.
spark = SparkSession.builder \
    .master("local[*]") \
    .appName("ColabNewsSentiment") \
    .config("spark.ui.showConsoleProgress", "false") \
    .getOrCreate()

sc = spark.sparkContext
sc.setLogLevel("ERROR")
print("Spark session started:", spark)

# 3) NEWS FETCHING (Real-Time Simulation)
# ---------------------------
# Strategy:
# - Default: fetch latest headlines from a public RSS feed (no API key needed).
#   Example: BBC News RSS feed: http://feeds.bbci.co.uk/news/rss.xml
# - Alternate (commented) option: use NewsAPI (requires an API key) or other paid services.
#
# Where to put API key: if you choose to use NewsAPI, place your key in NEWSAPI_KEY variable
# (see commented example below).
#
# Implementation: fetch_headlines() -> returns list of headline strings

def fetch_headlines_from_rss(rss_url="http://feeds.bbci.co.uk/news/rss.xml", max_items=50):
    """
    Fetches headlines from an RSS feed URL (public, no key).
    Returns a Python list of headline strings.
    """
    try:
        resp = requests.get(rss_url, timeout=10)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.content, "xml")
        items = soup.find_all("item")
        headlines = []
        for it in items[:max_items]:
            title_tag = it.find("title")
            if title_tag:
                headlines.append(title_tag.get_text().strip())
        return headlines
    except Exception as e:
        print("RSS fetch failed:", e)
        return []

# Example: how to use a real News API (commented)
# NEWSAPI_KEY = "<YOUR_NEWSAPI_KEY>"  # <-- put key here if you want to use NewsAPI
# def fetch_with_newsapi(query="world", page_size=50):
#     url = f"https://newsapi.org/v2/everything?q={query}&pageSize={page_size}&apiKey={NEWSAPI_KEY}"
#     r = requests.get(url)
#     data = r.json()
#     return [a['title'] for a in data.get('articles', [])]

# Initial fetch to seed headlines:
seed_headlines = fetch_headlines_from_rss(max_items=100)
print(f"Fetched {len(seed_headlines)} seed headlines (sample):")
for h in seed_headlines[:10]:
    print(" -", h)

# If no headlines were fetched (network issues), fallback to a small synthetic list:
if len(seed_headlines) == 0:
    print("No RSS headlines found - falling back to synthetic headlines for streaming.")
    seed_headlines = [
        "Government announces new economic measures to boost growth",
        "Local team wins dramatic overtime victory in championship",
        "Company reports mixed quarterly results as markets react",
        "Severe weather warnings issued across coastal regions",
        "Tech startup raises funds for AI-driven healthcare app",
        "Factory explosion injures several workers in industrial zone",
        "Scientists report breakthrough in renewable energy storage",
        "Protests erupt after controversial court ruling"
    ]

# ---------------------------

# 4) SYNTHETIC TRAINING DATA (small dataset generated programmatically)
# ---------------------------
# We create a small dataset of labeled headlines (Positive/Negative) programmatically,
# using templates to produce a few hundred examples quickly and reproducibly.
# This keeps the notebook self-contained and runnable immediately.

positive_templates = [
    "growth", "wins", "soars", "improves", "record high", "surge", "positive outlook",
    "successful", "recovery", "gain", "reform", "approval", "solves", "breakthrough", "expand"
]
negative_templates = [
    "loss", "crash", "decline", "fails", "downturn", "scandal", "controversy", "protest",
    "injures", "dies", "arrested", "attack", "warning", "concern", "shortage"
]

def generate_synthetic_headlines(n=300):
    """
    Generate n synthetic (headline, label) tuples.
    Label is 'Positive' or 'Negative'.
    """
    samples = []
    for i in range(n):
        if random.random() < 0.5:
            # Positive
            t = random.choice(positive_templates)
            headline = f"{random.choice(['Company','Market','Team','Government','City'])} {t} as {random.choice(['demand rises','policy works','efforts pay off','confidence returns'])}"
            label = "Positive"
        else:
            # Negative
            t = random.choice(negative_templates)
            headline = f"{random.choice(['Company','Market','Region','Hospital','Agency'])} {t} after {random.choice(['incident','reports','investigation','shortage','accident'])}"
            label = "Negative"
        samples.append((headline, label))
    return samples

# Generate synthetic training data
synthetic_data = generate_synthetic_headlines(n=400)
print("Generated synthetic training dataset size:", len(synthetic_data))
print("Sample examples:")
for s in synthetic_data[:6]:
    print(" -", s)

# 5) CREATE TRAINING SPARK DATAFRAME AND TRAIN PySpark ML PIPELINE
# ---------------------------
from pyspark.sql import Row
from pyspark.ml import Pipeline
from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

# Create Spark DataFrame from synthetic_data
rows = [Row(text=txt, label=lab) for txt, lab in synthetic_data]
train_df = spark.createDataFrame(rows)
train_df = train_df.select(col("text"), col("label"))

# Convert string labels ("Positive"/"Negative") into numeric index for ML algorithms
label_indexer = StringIndexer(inputCol="label", outputCol="labelIndex")

# Build a pipeline: Tokenizer -> StopWordsRemover -> HashingTF -> IDF -> LogisticRegression
tokenizer = Tokenizer(inputCol="text", outputCol="tokens")
stop_remover = StopWordsRemover(inputCol="tokens", outputCol="filtered_tokens")
hashing_tf = HashingTF(inputCol="filtered_tokens", outputCol="rawFeatures", numFeatures=1 << 12)  # 4096
idf = IDF(inputCol="rawFeatures", outputCol="features")
lr = LogisticRegression(featuresCol="features", labelCol="labelIndex", maxIter=50)

pipeline = Pipeline(stages=[label_indexer, tokenizer, stop_remover, hashing_tf, idf, lr])

# Fit the pipeline
print("Training pipeline on synthetic data (this may take a few seconds)...")
pipeline_model = pipeline.fit(train_df)
print("Pipeline trained.")

# Evaluate quickly on training data (sanity-check)
preds = pipeline_model.transform(train_df)
evaluator = MulticlassClassificationEvaluator(labelCol="labelIndex", predictionCol="prediction", metricName="accuracy")
acc = evaluator.evaluate(preds)
print(f"Training accuracy (synthetic data): {acc:.3f}")

# Map numerical predictions back to string labels:
# We need to know how StringIndexer mapped "Positive"/"Negative" -> numeric
label_mapping = pipeline_model.stages[0].labels  # list like ['Negative','Positive'] or vice versa
print("Label mapping (index -> label):", list(enumerate(label_mapping)))

def index_to_label(idx):
    # helper: convert numeric index to label string
    return label_mapping[int(idx)]

# 6) STREAMING SIMULATION: readStream('rate') + foreachBatch to apply pipeline_model
# ---------------------------
# Approach:
# - Use Spark Structured Streaming 'rate' source to generate sequential 'value' numbers.
# - In foreachBatch (which runs on driver), fetch latest headlines (using RSS), map value -> headline
# - Create a small batch DataFrame with columns 'text' and process it with pipeline_model.transform (batch-mode)
# - Aggregate results into a driver-side list for visualization
#
# Note: We intentionally use foreachBatch to keep model application simple and robust with PySpark ML.

from pyspark.sql.functions import expr

# Global store updated within foreachBatch; driver-only list for visualization
# Each element will be a dict with counts and timestamp
results_store = []  # driver-side list to store results per micro-batch

# Size of window for mapping rate.value -> headline index
headline_pool = seed_headlines.copy()  # initial seed headlines; foreachBatch will optionally refresh
headline_pool_len = len(headline_pool)
print("Using headline pool size:", headline_pool_len)

def foreach_batch_function(batch_df, batch_id):
    """
    This function will be called for each micro-batch by Structured Streaming.
    It runs on the driver; we:
      - fetch latest headlines (optional)
      - produce a list of headlines mapped from the 'value' column
      - create a Spark DataFrame of headlines and apply the trained pipeline_model
      - aggregate counts of Positive/Negative and append to results_store
    """
    global results_store, headline_pool, headline_pool_len

    # Convert micro-batch to Pandas to get 'value' list (small micro-batches expected)
    try:
        values = [int(r.value) for r in batch_df.select("value").collect()]
    except Exception as e:
        print("Error collecting batch values:", e)
        return

    if len(values) == 0:
        # nothing in this micro-batch
        return

    # Optionally fetch latest headlines from RSS on each batch to simulate 'real-time' new headlines
    try:
        fresh = fetch_headlines_from_rss(max_items=100)
        if len(fresh) > 0:
            headline_pool = fresh
            headline_pool_len = len(headline_pool)
    except Exception as e:
        # ignore fetch errors, keep existing pool
        pass

    # Map rate 'value's to headlines via modulo indexing
    batch_headlines = []
    for v in values:
        idx = int(v % headline_pool_len)
        batch_headlines.append((str(headline_pool[idx]),))  # single-column tuple

    # Create batch DataFrame with column 'text'
    batch_rows = [Row(text=h[0]) for h in batch_headlines]
    if len(batch_rows) == 0:
        return
    micro_df = spark.createDataFrame(batch_rows)

    # If pipeline_model expects a label column (it does during training) it's okay:
    # model.transform will produce predictions based on features.
    transformed = pipeline_model.transform(micro_df)

    # Collect predictions and compute counts
    pred_rows = transformed.select("text", "prediction").collect()
    pos_count = 0
    neg_count = 0
    details = []
    for r in pred_rows:
        label = index_to_label(r.prediction)
        details.append((r.text, label))
        if label == "Positive":
            pos_count += 1
        else:
            neg_count += 1

    # Append aggregated results (timestamp, counts, details) to results_store
    batch_time = time.time()
    results_store.append({
        "batch_id": int(batch_id),
        "ts": batch_time,
        "positive": pos_count,
        "negative": neg_count,
        "total": pos_count + neg_count,
        "details": details
    })

    # For debug: print small summary
    print(f"[batch {batch_id}] +{pos_count} pos / {neg_count} neg (total {pos_count + neg_count})")

# Create a streaming DataFrame using 'rate' to produce rows at runtime
# Each row has schema: timestamp, value
streaming_input = spark.readStream.format("rate").option("rowsPerSecond", 2).load()

# Start streaming query with foreachBatch sink
query = streaming_input.writeStream.foreachBatch(foreach_batch_function).start()
print("Structured streaming query started. Query id:", query.id)

# 7) REAL-TIME DASHBOARD (driver-side)
# ---------------------------
# We'll run a short live visualization loop that polls results_store and plots Positive vs Negative.
# The streaming query is running in background; foreachBatch will append to results_store regularly.

print("Starting live dashboard (will poll for 30 seconds). Press interrupt to stop sooner.")

display_interval = 1.0  # seconds between dashboard refresh
dashboard_duration = 30  # seconds for demonstration (adjust as desired)

start_time = time.time()
try:
    while time.time() - start_time < dashboard_duration:
        # Build aggregated counts from results_store
        total_pos = sum(item['positive'] for item in results_store)
        total_neg = sum(item['negative'] for item in results_store)
        total = total_pos + total_neg

        clear_output(wait=True)
        print(f"Real-time News Sentiment Dashboard (simulated) â€” running for {int(time.time()-start_time)}s")
        print(f"Total processed headlines: {total}    Positive: {total_pos}    Negative: {total_neg}\n")

        # Plot counts as a bar chart
        fig, ax = plt.subplots(figsize=(6,3))
        sns.barplot(x=["Positive", "Negative"], y=[total_pos, total_neg], ax=ax)
        ax.set_ylim(0, max(5, total_pos + total_neg))
        ax.set_title("Cumulative Sentiment Counts (simulated real-time)")
        for i, v in enumerate([total_pos, total_neg]):
            ax.text(i, v + 0.05, str(v), ha="center")
        plt.show()

        # Print last batch details (if present)
        if len(results_store) > 0:
            last = results_store[-1]
            print(f"Last batch (id={last['batch_id']}) processed at {time.ctime(last['ts'])}: "
                  f"{last['positive']} positive, {last['negative']} negative")
            print("Sample classifications from last batch:")
            for sample_text, sample_label in last["details"][:5]:
                print(" -", sample_label, ":", sample_text)

        time.sleep(display_interval)

except KeyboardInterrupt:
    print("Dashboard interrupted by user.")

# Spark ML provides its own methods for saving and loading models
pipeline_model.save("sentiment_pipeline_model")

print("âœ… Pipeline model saved successfully!")